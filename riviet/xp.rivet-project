version: 4
data:
  attachedData:
    trivet:
      testSuites:
        - id: azV4M3o4FSr-2aykCefr7
          name: Test Validator BMP
          testCases:
            - expectedOutput:
                AUTOR_NOME_COMPLETO: PATRÍCIA NASCIMENTO DE ALMEIDA
                autor_artigo: a Autora
                autor_artigo_sujeito: a Autora
                caps_autor_artigo_sujeito: A Autora
                dependency_alteracao_itinerario: ""
                dependency_avaria_bagagem_ausencia_demonstracao: ""
                dependency_avaria_bagagem_inversao_do_onus: ""
                dependency_inversao_do_onus: Fica
                destino_da_viagem: Istambul
                itinerario_completo: Rio de Janeiro – São Paulo - Istambul
                nome_da_acao: AÇÃO INDENIZATÓRIA POR DANOS MORAIS E MATERIAIS
                origem_da_viagem: Rio de Janeiro
                tag_autor: Autora
              id: n16HFB-5jZ0VPncKjZY8B
              input:
                enderecamento: EXCELENTISSIMO ETC
                peticao_inicial: file_input
          testGraph: OhbuUfcMpthjrriAEcVgD
          validationGraph: J7k0Em3n2Lbdqc5rEaoKL
      version: 1
  graphs:
    7ou7a2Ta97As0gb-5bS3r:
      metadata:
        description: ""
        id: 7ou7a2Ta97As0gb-5bS3r
        name: ChatWithFunctionsListOutput
      nodes:
        '[1Z1UCFEoaVyk6VpTrYrS_]:if "If"':
          data:
            unconnectedControlFlowExcluded: true
          outgoingConnections:
            - falseOutput->"Chat (Anthropic)" QBmQkeWBlmajMqUfzE7q2/prompt
            - output->"Chat" Z46AQNstKzHr-7P63L0GV/prompt
          visualData: -902.7917779157088/-883.1989330812557/155/297//
        '[Dqr_SJkFGoPhbV3R5YEVz]:code "Code"':
          data:
            code: >-
              // This is a code node, you can write and JS in here and it will
              be executed.

              // Inputs are accessible via an object `inputs` and data is typed (i.e. inputs.foo.type, inputs.foo.value)

              // Return an object with named outputs that match the output names specified in the node's config.

              // Output values must by typed as well (e.g. { bar: { type: 'string', value: 'bar' } }

              return {
                final_prompt: {
                  type: 'string',
                  value: inputs.raw_prompt.value.replace(">>template<<", "").replace(">>list_template<<", "").replace(">>explanation<<","").replace(":string","").replace(":dependency","").replace(":slashed_date","").replace(":currency","").replace(":quantity","").replace(":cpf","").replace(":cnpj","")
                }
              };
            inputNames:
              - raw_prompt
            outputNames:
              - final_prompt
          outgoingConnections:
            - final_prompt->"If" 1Z1UCFEoaVyk6VpTrYrS_/value
          visualData: -1738.4510974276357/-1226.5112012883828/230/284//
        '[GcwoUnVTpD2oUuoq2l6P8]:getGlobal "Global - Temperature"':
          data:
            dataType: any
            id: temperature
            onDemand: false
            useIdInput: false
            wait: true
          outgoingConnections:
            - value->"Chat (Anthropic)" QBmQkeWBlmajMqUfzE7q2/temperature
            - value->"Chat" Z46AQNstKzHr-7P63L0GV/temperature
          visualData: -1477.2612777208392/-1798.6678133939126/230/288//
        '[Ie8Zr1rqyaIuYEttIw1xR]:getGlobal "Global - Top P"':
          data:
            dataType: any
            id: top_p
            onDemand: false
            useIdInput: false
            wait: true
          outgoingConnections:
            - value->"Chat (Anthropic)" QBmQkeWBlmajMqUfzE7q2/top_p
            - value->"Chat" Z46AQNstKzHr-7P63L0GV/top_p
          visualData: -1478.7114185395783/-1600.6469774193963/230/287//
        '[PAMM6BFWrM0hDQMs2n9P6]:graphOutput "Graph Output"':
          data:
            dataType: string[]
            id: list_of_objects
          visualData: 808.5887463493265/-1098.2490878312778/330/314//
        '[PAjFWzilgzR6mud0g4MXQ]:extractObjectPath "Extract Object Path"':
          data:
            path: $.arguments.desired_items_list
            usePathInput: false
          outgoingConnections:
            - match->"If/Else" km99zkMz1vTw8yfnCduBf/false
          visualData: 146.47717850492984/-739.5749427075738/280/336//
        '[Q7Ws__Lkp8XhpurBnPeUR]:getGlobal "Global - Max Tokens"':
          data:
            dataType: any
            id: max_tokens
            onDemand: false
            useIdInput: false
            wait: true
          outgoingConnections:
            - value->"Chat (Anthropic)" QBmQkeWBlmajMqUfzE7q2/maxTokens
            - value->"Chat" Z46AQNstKzHr-7P63L0GV/maxTokens
          visualData: -1470.9151557647185/-1419.0373790815852/230/286//
        '[QBmQkeWBlmajMqUfzE7q2]:chatAnthropic "Chat (Anthropic)"':
          data:
            cache: false
            maxTokens: 1024
            model: claude-2
            stop: ""
            temperature: 0.5
            top_p: 1
            useAsGraphPartialOutput: true
            useMaxTokensInput: true
            useModelInput: true
            useStop: false
            useStopInput: false
            useTemperatureInput: true
            useTopKInput: false
            useTopP: false
            useTopPInput: true
            useUseTopPInput: false
          outgoingConnections:
            - response->"Code" lRvv4U1h87gCpuYcN0Fgf/llm_result
          visualData: -505.3918164032983/-900.0763027964774/305/337//
        '[T6SekL2P77vAlf64SVGbk]:getGlobal "Global - System Role"':
          data:
            dataType: string
            id: system_role
            onDemand: false
            useIdInput: false
            wait: true
          outgoingConnections:
            - value->"Chat" Z46AQNstKzHr-7P63L0GV/systemPrompt
          visualData: -1246.9704266056833/-2162.6118144223638/230/222//
        '[YJMpgouwVAFjjc7U4Nz50]:graphInput "Graph Input"':
          data:
            dataType: string
            id: input
            useDefaultValueInput: true
          outgoingConnections:
            - data->"Code" Dqr_SJkFGoPhbV3R5YEVz/raw_prompt
            - data->"Code" _qaCU-7ytgo9ywiUtL1bo/raw_prompt
          visualData: -2215.764585717574/-1160.351991420572/330/283//
        '[Z46AQNstKzHr-7P63L0GV]:chat "Chat"':
          data:
            cache: false
            code: ""
            enableFunctionUse: true
            frequencyPenalty: 0
            headers: []
            maxTokens: 1024
            model: gpt-3.5-turbo-16k-0613
            presencePenalty: 0
            responseFormat: ""
            stop: ""
            temperature: 0.5
            toolChoice: function
            toolChoiceFunction: get_json
            top_p: 1
            useAsGraphPartialOutput: true
            useFrequencyPenaltyInput: false
            useMaxTokensInput: true
            useModelInput: true
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: true
            useTopP: false
            useTopPInput: true
            useUseTopPInput: false
            useUserInput: false
          outgoingConnections:
            - function-call->"Extract Object Path" xOm_GPWEC54ndHHx0--fF/object
          visualData: -524.5068247740273/-1319.3140023386943/230/338//
        '[_qaCU-7ytgo9ywiUtL1bo]:code "Code"':
          data:
            code: >-
              // This is a code node, you can write and JS in here and it will
              be executed.

              // Inputs are accessible via an object `inputs` and data is typed (i.e. inputs.foo.type, inputs.foo.value)

              // Return an object with named outputs that match the output names specified in the node's config.

              // Output values must by typed as well (e.g. { bar: { type: 'string', value: 'bar' } }

              let raw_prompt = inputs.raw_prompt.value


              let functionsJson = {}

              functionsJson["type"] = "object"

              functionsJson["properties"] = {}

              functionsJson["required"] = []

              if (raw_prompt.includes(">>list_template<<")) {
                  functionsJson["properties"]["desired_items_list"] = {
                      "type": "array",
                      "items": {
                          "type": "object",
                          "required": [],
                          "properties": {}
                      }
                  }
                  functionsJson["required"].push("desired_items_list")
                  let listTemplateLines = raw_prompt.split("\n").filter(l => l.includes(">>list_template<<"))
                  for (let idx = 0; idx < listTemplateLines.length; idx++) {
                      let templateName = listTemplateLines[idx].match(/"([^']+)"/)[1]
                      functionsJson["properties"]["desired_items_list"]["items"]["required"].push(templateName)
                      functionsJson["properties"]["desired_items_list"]["items"]["properties"][templateName] = {
                          "type": "string"
                      }
                  }
              }

              else if (raw_prompt.includes(">>template<<")) {
                  let templateLines = raw_prompt.split("\n").filter(l => l.includes(">>template<<"))
                  for (let idx = 0; idx < templateLines.length; idx++) {
                      let templateName = templateLines[idx].match(/"([^']+)"/)[1]
                      functionsJson["properties"][templateName] = {
                          "type": "string"
                      }
                      functionsJson["required"].push(templateName)
                  }
              }




              return {
                openai_functions: {
                  type: "object",
                  value: functionsJson
                }
              };
            inputNames:
              - raw_prompt
            outputNames:
              - openai_functions
          outgoingConnections:
            - openai_functions->"GPT Function" wab9nEJB3yOPoE8hPlEG3/schema
          visualData: -1714.8530836285302/-811.3461865877446/230/325//
        '[e3WNkorSD6CG-1CFwdzjM]:code "Code"':
          data:
            code: >-
              // This is a code node, you can write and JS in here and it will
              be executed.

              // Inputs are accessible via an object `inputs` and data is typed (i.e. inputs.foo.type, inputs.foo.value)

              // Return an object with named outputs that match the output names specified in the node's config.

              // Output values must by typed as well (e.g. { bar: { type: 'string', value: 'bar' } }

              let is_open_ai = inputs.model_name.value.includes("gpt") ? true : false


              return {
                is_open_ai: {
                  type: 'boolean',
                  value: is_open_ai
                }
              };
            inputNames:
              - model_name
            outputNames:
              - is_open_ai
          outgoingConnections:
            - is_open_ai->"If" 1Z1UCFEoaVyk6VpTrYrS_/if
            - is_open_ai->"If/Else" km99zkMz1vTw8yfnCduBf/if
          visualData: -1175.5786576926912/-1242.4792137631327/230/295//
        '[ef6zzPTD9fDvAcZnZklom]:graphInput "Graph Input"':
          data:
            dataType: string
            defaultValue: Get entities list as defined in the JSON schema
            id: openai_functions_description
            schema: ""
            useDefaultValueInput: false
          outgoingConnections:
            - data->"GPT Function" wab9nEJB3yOPoE8hPlEG3/description
          visualData: -2214.2266864197527/-430.4415999999993/330/309//
        '[km99zkMz1vTw8yfnCduBf]:ifElse "If/Else"':
          data:
            unconnectedControlFlowExcluded: true
          outgoingConnections:
            - output->"Graph Output" PAMM6BFWrM0hDQMs2n9P6/value
          visualData: 308.1158347528474/-1061.5084057159663/205/null//
        '[lRvv4U1h87gCpuYcN0Fgf]:code "Code"':
          data:
            code: >-
              // This is a code node, you can write and JS in here and it will
              be executed.

              // Inputs are accessible via an object `inputs` and data is typed (i.e. inputs.foo.type, inputs.foo.value)

              // Return an object with named outputs that match the output names specified in the node's config.

              // Output values must by typed as well (e.g. { bar: { type: 'string', value: 'bar' } }


              // Find the JSON within the input string

              const llmResultString = JSON.stringify(inputs.llm_result.value)


              let startIndex = llmResultString.indexOf("[");

              let endIndex = llmResultString.lastIndexOf("]");


              if (startIndex != -1 && endIndex != -1) {
                let jsonStr = llmResultString.slice(startIndex, endIndex + 1);
                // Attempt to parse the JSON
                jsonStr = jsonStr.replaceAll('\\n', '\n'); // replace occurrences of \n
                jsonStr = jsonStr.replaceAll('\\"', '"'); // replace occurrences of \n
                const llmProcessedObject = {
                  "arguments": {
                    "desired_items_list": JSON.parse(jsonStr)
                  }
                }
                return {
                  output: {
                    value: llmProcessedObject,
                    type: 'object'
                  }
                }
              }

              else {
                throw new Error("No valid JSON found in the input string.");
              }
            inputNames:
              - llm_result
            outputNames:
              - output
          outgoingConnections:
            - output->"Extract Object Path" PAjFWzilgzR6mud0g4MXQ/object
          visualData: -144.993077985039/-739.5749427075732/230/339//
        '[oQYmiHOcM3ZES1m4dUh0K]:getGlobal "Global - Model Name"':
          data:
            dataType: string
            id: model_name
            onDemand: false
            useIdInput: false
            wait: true
          outgoingConnections:
            - value->"Chat (Anthropic)" QBmQkeWBlmajMqUfzE7q2/model
            - value->"Chat" Z46AQNstKzHr-7P63L0GV/model
            - value->"Code" e3WNkorSD6CG-1CFwdzjM/model_name
          visualData: -1481.1679429020196/-2011.3942644869555/230/289//
        '[wab9nEJB3yOPoE8hPlEG3]:gptFunction "GPT Function"':
          data:
            description: Get entities list as defined in the JSON schema
            name: get_json
            schema: |-
              {
                "type": "object",
                "properties": {}
              }
            useDescriptionInput: true
            useNameInput: false
            useSchemaInput: true
          outgoingConnections:
            - function->"Chat" Z46AQNstKzHr-7P63L0GV/functions
          visualData: -1240.733906172839/-750.192553086419/280/310//
        '[xOm_GPWEC54ndHHx0--fF]:extractObjectPath "Extract Object Path"':
          data:
            code: ""
            path: $.arguments.desired_items_list
            usePathInput: false
          outgoingConnections:
            - match->"If/Else" km99zkMz1vTw8yfnCduBf/true
          visualData: -250.02009260921454/-1265.462094788866/280/331//
    AyQeZ4we5T_SGin85M5XS:
      metadata:
        description: ""
        id: AyQeZ4we5T_SGin85M5XS
        name: LogOutputToLambda
      nodes:
        '[2u9g8N-TXN4L2WrZl79ZQ]:text "FALSE"':
          data:
            text: "false"
          outgoingConnections:
            - output->"Compare" DxuySxynUQWOurAA8NBG1/b
          visualData: 304.4210804291502/253.33107107697646/330/61//
        '[DxuySxynUQWOurAA8NBG1]:compare "Compare"':
          data:
            comparisonFunction: ==
          outgoingConnections:
            - output->"If" hXT34vECs3KcGDqmWfS6_/if
          visualData: 853.7109034779244/87.71727809005642/190/55//
        '[I6AGcjx1T1IGBklgOrF_X]:graphInput "Graph Input"':
          data:
            dataType: string
            id: model_name
            useDefaultValueInput: false
          outgoingConnections:
            - data->"If" hXT34vECs3KcGDqmWfS6_/value
          visualData: 354/452/330/48//
        '[J8VdrEGyvf887NAXSQl1o]:graphInput "Graph Input"':
          data:
            dataType: string
            id: prompt_text
            useDefaultValueInput: false
          visualData: 407.5132467462262/859.9470813006138/330/64//
        '[d7j3-WPo8JwO6bLxIPljH]:getGlobal "Get Global"':
          data:
            dataType: string
            id: is_running_on_local_rivet_app
            onDemand: false
            useIdInput: false
            wait: true
          outgoingConnections:
            - value->"Compare" DxuySxynUQWOurAA8NBG1/a
          visualData: 427.7051756638285/13.086194366060894/230/59//
        '[hXT34vECs3KcGDqmWfS6_]:if "If"':
          data:
            unconnectedControlFlowExcluded: true
          outgoingConnections:
            - output->"External Call" v-kgANCrLChP_gfscqDcO/arguments
          visualData: 874.9506172839506/471.2901234567902/155/null//
        '[v-kgANCrLChP_gfscqDcO]:externalCall "External Call"':
          data:
            functionName: print
            useErrorOutput: false
            useFunctionNameInput: false
          visualData: 1228.9876543209875/468.01234567901236/193.61464652173368/49//
        '[xyyvDmy3FX7tNlHNXS8Oi]:graphInput "GRAPH INPUT"':
          data:
            dataType: string
            id: entry
            useDefaultValueInput: false
          visualData: 414.0243309346732/686.3181696086962/330/63//
    CFtE0iCYza8rNiWbag0IW:
      metadata:
        description: ""
        id: CFtE0iCYza8rNiWbag0IW
        name: GetFinalAiReasonsJson
      nodes:
        '[-1DqHmVdokGuSh4ryZh3S]:graphInput "Graph Input"':
          data:
            dataType: object[]
            id: list_of_jsons_2
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Code" woz8QGcPH7gI108R87Sip/list_of_jsons_2
          visualData: 413.89629629629627/1088.3833333333332/330/10//
        '[1bpzS8e3UKo80xasRuRrS]:graphInput "Graph Input"':
          data:
            dataType: object[]
            defaultValue: ""
            id: list_of_jsons_1
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Code" woz8QGcPH7gI108R87Sip/list_of_jsons_1
          visualData: 389.89629629629627/909.3833333333332/330/28//
        '[AwUAE9YU1j8Q8ZBpDuXwJ]:graphInput "Graph Input"':
          data:
            dataType: object
            id: json_4
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Code" woz8QGcPH7gI108R87Sip/json_4
          visualData: 369.13600823045266/596.8637174211249/330/21//
        '[FuUmjDmKcav_GoLWVGY0A]:graphInput "Graph Input"':
          data:
            dataType: object[]
            defaultValue: ""
            id: list_of_jsons_3
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Code" woz8QGcPH7gI108R87Sip/list_of_jsons_3
          visualData: 413.89629629629627/1288.3833333333332/330/10//
        '[NoRBtD24d8wdjNncvUnYk]:graphInput "Graph Input"':
          data:
            dataType: object[]
            defaultValue: ""
            id: list_of_jsons_4
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Code" woz8QGcPH7gI108R87Sip/list_of_jsons_4
          visualData: 413.89629629629627/1488.3833333333332/330/10//
        '[PEutEVU2awGH2y-r8x_Rt]:graphInput "Graph Input"':
          data:
            dataType: object
            id: json_3
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Code" woz8QGcPH7gI108R87Sip/json_3
          visualData: 374.63175582990397/444.9515089163238/330/20//
        '[Pm2dlBWe7o7JPi-3P10oo]:graphInput "Graph Input"':
          data:
            dataType: object
            id: json_5
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Code" woz8QGcPH7gI108R87Sip/json_5
          visualData: 369.13600823045266/747.4019890260631/330/22//
        '[VruB3e5eBVokizn5zhSky]:graphOutput "Graph Output"':
          data:
            dataType: object
            id: final_ai_reasons_json
          visualData: 1533.866392318244/742.645061728395/330/13//
        '[nh4GwY7cwom0oybaL0uJ5]:graphInput "Graph Input"':
          data:
            dataType: object
            id: json_7
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Code" woz8QGcPH7gI108R87Sip/json_7
          visualData: -58.86399176954734/285.4019890260631/330/30//
        '[obvC_dmpXU8JtfPVFrcNn]:graphInput "Graph Input"':
          data:
            dataType: object
            id: json_6
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Code" woz8QGcPH7gI108R87Sip/json_6
          visualData: -58.86399176954734/146.40198902606312/330/29//
        '[ph_P93R5I3WzdN8WLovV_]:graphInput "Graph Input"':
          data:
            dataType: object
            id: json_2
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Code" woz8QGcPH7gI108R87Sip/json_2
          visualData: 373.35411522633746/279.21550068587106/330/24//
        '[ptT2qFJExMEWIfWrnRzYC]:graphInput "Graph Input"':
          data:
            dataType: object
            defaultValue: ""
            id: json_1
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Code" woz8QGcPH7gI108R87Sip/json_1
          visualData: 437.832853223594/122.08518518518521/330/18//
        '[qgsDzksN0-hJ2ItV06HBr]:graphInput "Graph Input"':
          data:
            dataType: object
            id: json_9
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Code" woz8QGcPH7gI108R87Sip/json_9
          visualData: -61.86399176954734/580.4019890260631/330/32//
        '[wXobYiH2-nJlRlKqEb1lM]:graphInput "Graph Input"':
          data:
            dataType: object
            id: json_8
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Code" woz8QGcPH7gI108R87Sip/json_8
          visualData: -61.86399176954734/424.4019890260631/330/31//
        '[woz8QGcPH7gI108R87Sip]:code "Code"':
          data:
            code: >-
              let list_of_inputs = [
                  inputs.json_1.value,
                  inputs.json_2.value,
                  inputs.json_3.value,
                  inputs.json_4.value,
                  inputs.json_5.value,
                  inputs.json_6.value,
                  inputs.json_7.value,
                  inputs.json_8.value,
                  inputs.json_9.value,
                  inputs.list_of_jsons_1.value,
                  inputs.list_of_jsons_2.value,
                  inputs.list_of_jsons_3.value,
                  inputs.list_of_jsons_4.value
              ]


              function numerate_dict_keys(dict_info, number) {
                  const numerated_dict = {};
                  for (const key in dict_info) {
                      numerated_dict[`${key}${number}`] = dict_info[key];
                  }
                  return numerated_dict;
              }

              let final_ai_reasons_json_dict = {}



              for (let i = 0; i < list_of_inputs.length; i++) {
                  node_output = list_of_inputs[i]
                  if (node_output !== undefined) {
                      first_element_of_node_output = node_output[0]
                      if (typeof (first_element_of_node_output) !== "object") {
                          for (const key in node_output) {
                              final_ai_reasons_json_dict[key] = node_output[key];
                          }
                      }
                      else {


                          for (let j = 0; j < node_output.length; j++) {
                              list_of_jsons = node_output[j]
                              let numerated_desired_items_list = numerate_dict_keys(list_of_jsons, j + 1)
                              for (const key in numerated_desired_items_list) {
                                  final_ai_reasons_json_dict[key] = numerated_desired_items_list[key];
                              }
                          }

                      }
                  }
              }




              return {
                final_ai_reasons_json: {
                  type: typeof(final_ai_reasons_json_dict),
                  value: final_ai_reasons_json_dict
                }
              };
            inputNames:
              - json_1
              - json_2
              - json_3
              - json_4
              - json_5
              - json_6
              - json_7
              - json_8
              - json_9
              - list_of_jsons_1
              - list_of_jsons_2
              - list_of_jsons_3
              - list_of_jsons_4
            outputNames:
              - final_ai_reasons_json
          outgoingConnections:
            - final_ai_reasons_json->"Graph Output" VruB3e5eBVokizn5zhSky/value
          visualData: 1150.8624828532236/549.6745541838134/230/11//
    No4XBpiQKMTWMDJ-0Cwpy:
      metadata:
        description: ""
        id: No4XBpiQKMTWMDJ-0Cwpy
        name: TrimPrompt
      nodes:
        '[4hA6lO9qOn5A0F6KP6-ls]:subGraph "Subgraph"':
          data:
            graphId: k0HWnuSssHo77oxIe13DD
            useAsGraphPartialOutput: false
            useErrorOutput: false
          outgoingConnections:
            - output->"Code" bSGheCy8beG2_68AyXT3D/prompt_token_size
          visualData: 754.7892237645166/269.96423731092835/330/31//
        '[E5RzYwAiCG2JvzunldySw]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: prompt_text_with_entry
          visualData: 1868.504798820617/447.5830368749581/330/29//
        '[aKaIZZfy-CotKPoIeoiUQ]:graphInput "Graph Input"':
          data:
            dataType: string
            id: entry_text
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Code" bSGheCy8beG2_68AyXT3D/entry_text
            - data->"Subgraph" hx-L-OLtdGZLRYwOt6CYo/text
          visualData: 203.99074715579488/647.6939740861047/330/26//
        '[bSGheCy8beG2_68AyXT3D]:code "Code"':
          data:
            code: >
              // This functions sums the prompt and entry text tokens quantity


              // INPUTS

              let prompt_text = inputs.prompt_text.value;

              let entry_text = inputs.entry_text.value;

              let prompt_token_size = inputs.prompt_token_size.value;

              let entry_token_size = inputs.entry_token_size.value;

              let max_tokens = inputs.max_tokens.value;

              let model = inputs.model.value;


              // CONSTANTS

              const entry_pattern = /<<([^>]+)>>/;

              const correctionFactor = 0.8

              const modelsTokenLimitDict = {
                  "gpt-4-1106-preview": parseInt(128000 * correctionFactor),
                  "gpt-4": parseInt(8192 * correctionFactor),
                  "claude-2": parseInt(100000 * correctionFactor),
                  "gpt-3.5-turbo-16k": parseInt(16385 * correctionFactor),
              };


              let total_token_sum = prompt_token_size + entry_token_size + max_tokens


              if (total_token_sum > modelsTokenLimitDict[model]) {
                let difference_in_tokens = total_token_sum - modelsTokenLimitDict[model]
                let difference_in_text_length = difference_in_tokens * 4
                if (difference_in_text_length > (entry_text.length)) {
                  difference_in_text_length = difference_in_tokens * 3
                }
                entry_text = entry_text.slice(0, -difference_in_text_length)
              } 


              prompt_with_replace = prompt_text.replace(entry_pattern, entry_text)


              return {
                  output1: {
                    type: 'string',
                    value: prompt_with_replace
                  },
                  token_limit: {
                    type: 'number',
                    value: modelsTokenLimitDict[model]
                  }
                };
            inputNames:
              - prompt_text
              - entry_text
              - prompt_token_size
              - entry_token_size
              - max_tokens
              - model
            outputNames:
              - output1
              - token_limit
          outgoingConnections:
            - output1->"Graph Output" E5RzYwAiCG2JvzunldySw/value
          visualData: 1232.0725210575772/330.23097789782946/393.22101351278/28//
        '[dN9_DXNpY6F9HcXBhOtOy]:graphInput "Graph Input"':
          data:
            dataType: number
            id: max_tokens
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Code" bSGheCy8beG2_68AyXT3D/max_tokens
          visualData: 672.6145553413895/786.3459603133/330/16//
        '[gO8JEC7OaE9ZN78Xg3KFU]:graphInput "Graph Input"':
          data:
            dataType: string
            id: promp_text
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Code" bSGheCy8beG2_68AyXT3D/prompt_text
            - data->"Subgraph" 4hA6lO9qOn5A0F6KP6-ls/text
          visualData: 288.8313945381036/342.9982573568215/330/24//
        '[hx-L-OLtdGZLRYwOt6CYo]:subGraph "Subgraph"':
          data:
            graphId: k0HWnuSssHo77oxIe13DD
            useAsGraphPartialOutput: false
            useErrorOutput: false
          outgoingConnections:
            - output->"Code" bSGheCy8beG2_68AyXT3D/entry_token_size
          visualData: 613.1728698194144/560.9120269363775/330/25//
        '[uyj8e8FCbGOqPhldBVk7Z]:graphInput "Graph Input"':
          data:
            dataType: string
            id: model
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Code" bSGheCy8beG2_68AyXT3D/model
          visualData: 669.0738015148012/1024.7090088471368/330/30//
    VBfwPp3tXdEmMiB6_sXzD:
      metadata:
        description: ""
        id: VBfwPp3tXdEmMiB6_sXzD
        name: DeployTemplateNameAndSubgraph
      nodes:
        '[-qUQHc4IN-aD399iZFO7_]:text "Get TemplateId By Name"':
          data:
            text: "{{base_api_url}}/template-id-from-name"
          outgoingConnections:
            - output->"API CALL- GET TEMPLATEID BY NAME"
              OB-3rHEDcVGO96UtMqecU/url
          visualData: 233/719/330/12//
        '[1YMj3H7yPXVrj_OzRK_8i]:getGlobal "Get Global"':
          data:
            dataType: string
            id: rivet_project_file_name
            onDemand: false
            useIdInput: false
            wait: true
          outgoingConnections:
            - value->"If" OLPU-gcaOVGEHDcM-bdTp/value
          visualData: -800.8247563162075/1683.4281267046774/230/332//
        '[1sbmsaD5JjVzc83rEEzC5]:text "Headers - Create Prompt"':
          data:
            text: |-
              {
                "template_id": "{{template_id}}",
                "prompt_group_name": "{{prompt_group_name}}",
                "engine": "{{engine}}",
                "model": "{{model}}",
                "temperature": "{{temperature}}",
                "top_p": "{{top_p}}",
                "max_tokens": "{{max_tokens}}",
                "system_role": "{{system_role}}",
                "rivet_project_s3_path": "{{rivet_project_s3_path}}",
                "rivet_graph_name": "{{rivet_graph_name}}",
                "Content-Type": "text/plain",
                "Authorization": "{{authorization}}"
              }
          outgoingConnections:
            - output->"API CALL- CREATE PROMPT" KrrYfSpgD7ekZyU1Etmta/headers
          visualData: 1745/1599/330/37//
        '[2fzam2l5PLsoUIDnB68Ku]:text "Backoffice - Base API URL"':
          data:
            text: https://backoffice.talismanai.co
          outgoingConnections:
            - output->"Create/Update Prompt" P5_ai97jCAACPa4by5ZeW/base_api_url
            - output->"Get TemplateId By Name" -qUQHc4IN-aD399iZFO7_/base_api_url
            - output->"Upload Rivet Project to S3"
              KmNS5D0_Z3l6iPOi9icqg/base_api_url
          visualData: -200/691/330/17//
        '[3Nyw_z28AjMgsmQJ_LnBi]:extractObjectPath "Extract Object Path"':
          data:
            path: $.template_id
            usePathInput: false
          outgoingConnections:
            - match->"Headers - Create Prompt" 1sbmsaD5JjVzc83rEEzC5/template_id
          visualData: 1447/904/280/22//
        '[3ot3n0IxQCXakyA07tXug]:ifElse "If/Else"':
          data:
            unconnectedControlFlowExcluded: true
          visualData: -424/1701/205/null//
        '[7lntd5mMhSJDKoNNygDl7]:getGlobal "Global - Temperature"':
          data:
            dataType: any
            id: temperature
            onDemand: false
            useIdInput: false
            wait: true
          outgoingConnections:
            - value->"Headers - Create Prompt" 1sbmsaD5JjVzc83rEEzC5/temperature
          visualData: 898/1812/230/255//
        '[8QM3LOf9PhxR6TsSXJFRS]:text "Dummy Body Rivet Graph Prompt"':
          data:
            text: |-
              Rivet Graph

              >>template<<"dummy_json_template":string
          outgoingConnections:
            - output->"API CALL- CREATE PROMPT" KrrYfSpgD7ekZyU1Etmta/req_body
          visualData: 2429.6565148971195/1723.2336329218108/330/321//
        '[AfKBwfMCgfWHiiPVqvLUe]:getGlobal "Get Global"':
          data:
            dataType: string
            id: template_name
            onDemand: false
            useIdInput: false
            wait: true
          outgoingConnections:
            - value->"If" zsBFtbd9Xwr5YNUsuBOnJ/value
          visualData: -1386.4592156478993/561.4595027438259/230/331//
        '[CWQvxx8D1qcVh_XhebpCb]:object "Headers Auth"':
          data:
            jsonTemplate: |-
              {
                "Content-Type": "text/plain",
                "Authorization": "{{auth_token}}"
              }
          outgoingConnections:
            - output->"API CALL- GET TEMPLATEID BY NAME"
              OB-3rHEDcVGO96UtMqecU/headers
            - output->"API CALL- UPLOAD RIVET PROJECT TO S3"
              PhIuLXlWS-S1luwv-T9W3/headers
            - output->"Extract Object Path" m190e98AhL7wN5UJIIFkE/object
          visualData: -492/543/230/323//
        '[HNpYI-lWhgdH83hhS0fnh]:extractObjectPath "Extract Object Path"':
          data:
            path: $.path
            usePathInput: false
          outgoingConnections:
            - match->"Headers - Create Prompt"
              1sbmsaD5JjVzc83rEEzC5/rivet_project_s3_path
          visualData: 1795/1400/280/34//
        '[JSDyQgkFfSKHhEFs5gRm2]:extractJson "Extract JSON"':
          outgoingConnections:
            - output->"Extract Object Path" 3Nyw_z28AjMgsmQJ_LnBi/object
          visualData: 1099/944/280/23//
        '[KTd5BXo5s7WJCfRGrStCQ]:getGlobal "Global - Top P"':
          data:
            dataType: any
            id: top_p
            onDemand: false
            useIdInput: false
            wait: true
          outgoingConnections:
            - value->"Headers - Create Prompt" 1sbmsaD5JjVzc83rEEzC5/top_p
          visualData: 901/2224/230/289//
        '[KmNS5D0_Z3l6iPOi9icqg]:text "Upload Rivet Project to S3"':
          data:
            text: "{{base_api_url}}/upload-rivet-project-to-s3"
          outgoingConnections:
            - output->"API CALL- UPLOAD RIVET PROJECT TO S3"
              PhIuLXlWS-S1luwv-T9W3/url
          visualData: 553/1227/330/52//
        '[KrrYfSpgD7ekZyU1Etmta]:httpCall "API CALL- CREATE PROMPT"':
          data:
            body: ""
            errorOnNon200: true
            headers: ""
            method: POST
            url: ""
            useBodyInput: true
            useHeadersInput: true
            useUrlInput: true
          outgoingConnections:
            - res_body->"If/Else" xoF9FGaSpU79K4oAK835D/true
          visualData: 2341.1111111111113/1231.7777777777778/280/38//
        '[NIVBioBOV9dWjmO0bpyPv]:getGlobal "Global - Model Name"':
          data:
            dataType: string
            id: model_name
            onDemand: false
            useIdInput: false
            wait: true
          outgoingConnections:
            - value->"Compare" fywYLAj_waj20lE-PRaJk/a
            - value->"Headers - Create Prompt" 1sbmsaD5JjVzc83rEEzC5/model
          visualData: -213/2014/230/303//
        '[NrbZzX1WqnHVDTh2qdU_2]:extractJson "Extract JSON"':
          outgoingConnections:
            - output->"Extract Object Path" HNpYI-lWhgdH83hhS0fnh/object
          visualData: 1455/1399/280/33//
        '[OB-3rHEDcVGO96UtMqecU]:httpCall "API CALL- GET TEMPLATEID BY NAME"':
          data:
            body: ""
            errorOnNon200: true
            headers: ""
            method: POST
            url: ""
            useBodyInput: true
            useHeadersInput: true
            useUrlInput: true
          outgoingConnections:
            - res_body->"Extract JSON" JSDyQgkFfSKHhEFs5gRm2/input
          visualData: 714/844/280/null//
        '[OLPU-gcaOVGEHDcM-bdTp]:if "If"':
          data:
            unconnectedControlFlowExcluded: true
          outgoingConnections:
            - output->"Headers - Create Prompt"
              1sbmsaD5JjVzc83rEEzC5/prompt_group_name
            - output->"Upload Rivet Project to S3 Payload"
              VFRDxpzz3DI2q-26gGQZe/rivet_project_file_name
          visualData: -75.96030156378652/1699.9064190946501/155/null//
        '[P5_ai97jCAACPa4by5ZeW]:text "Create/Update Prompt"':
          data:
            text: "{{base_api_url}}/prompt"
          outgoingConnections:
            - output->"API CALL- CREATE PROMPT" KrrYfSpgD7ekZyU1Etmta/url
          visualData: 1756/1201.6666666666667/330/39//
        '[PWLOFYAvrMGNAbK5iGQSB]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: output
          visualData: 3198/1297/330/null//
        '[PhIuLXlWS-S1luwv-T9W3]:httpCall "API CALL- UPLOAD RIVET PROJECT TO S3"':
          data:
            body: ""
            errorOnNon200: true
            headers: ""
            method: POST
            url: ""
            useBodyInput: true
            useHeadersInput: true
            useUrlInput: true
          outgoingConnections:
            - res_body->"Extract JSON" NrbZzX1WqnHVDTh2qdU_2/input
          visualData: 1028/1345/280/51//
        '[QY1s4zZ6PMEsNVPu9Hs5P]:getGlobal "Global - Max Tokens"':
          data:
            dataType: any
            id: max_tokens
            onDemand: false
            useIdInput: false
            wait: true
          outgoingConnections:
            - value->"Headers - Create Prompt" 1sbmsaD5JjVzc83rEEzC5/max_tokens
          visualData: 898/2404/230/292//
        '[QdxPzaFvTu2f3E9DNMO-X]:getGlobal "Get Global"':
          data:
            dataType: string
            id: backoffice_user_password
            onDemand: false
            useIdInput: false
            wait: true
          outgoingConnections:
            - value->"Code" _VjbaAnKngTINcRZBum5H/backoffice_user_password
          visualData: -1386.4592156478993/361.4595027438259/230/331//
        '[Re2CGu4B8XHrkV4f_Hanm]:getGlobal "Get Global"':
          data:
            dataType: string
            id: backoffice_user_email
            onDemand: false
            useIdInput: false
            wait: true
          outgoingConnections:
            - value->"Code" _VjbaAnKngTINcRZBum5H/backoffice_user_email
          visualData: -1166.2788181472245/135.35564118446752/230/330//
        '[U-7z7gVYzYEeq-Lg1XS8c]:text "claude-2"':
          data:
            text: claude-2
          outgoingConnections:
            - output->"Compare" fywYLAj_waj20lE-PRaJk/b
          visualData: -386/1886/330/304//
        '[VFRDxpzz3DI2q-26gGQZe]:object "Upload Rivet Project to S3 Payload"':
          data:
            jsonTemplate: |-
              {
                "rivet_project_file_content": "{{rivet_project_file_content}}",
                "rivet_project_file_name": "{{rivet_project_file_name}}"
              }
          outgoingConnections:
            - output->"API CALL- UPLOAD RIVET PROJECT TO S3"
              PhIuLXlWS-S1luwv-T9W3/req_body
          visualData: 566/1431/230/53//
        '[WLzdVbuF_JC7_D_883Axm]:graphInput "Deploy"':
          data:
            dataType: boolean
            id: deploy
            useDefaultValueInput: false
          outgoingConnections:
            - data->"If" OLPU-gcaOVGEHDcM-bdTp/if
            - data->"If" w-3MQdvsDtSC7lvVfpS-G/if
            - data->"If" zsBFtbd9Xwr5YNUsuBOnJ/if
            - data->"If/Else" xoF9FGaSpU79K4oAK835D/if
          visualData: -991/905/330/333//
        '[X4xeu1TKozYb5hUSH_W7M]:text "openai"':
          data:
            text: openai
          outgoingConnections:
            - output->"Model Engine" hnvR2MiDSp7M5djmO6GRY/false
          visualData: 23/2396/330/307//
        '[_VjbaAnKngTINcRZBum5H]:code "Code"':
          data:
            code: >-
              // This is a code node, you can write and JS in here and it will
              be executed.

              // Inputs are accessible via an object `inputs` and data is typed (i.e. inputs.foo.type, inputs.foo.value)

              // Return an object with named outputs that match the output names specified in the node's config.

              // Output values must by typed as well (e.g. { bar: { type: 'string', value: 'bar' } }

              return {
                auth_token: {
                  type: "string",
                  value: "Bearer "+ btoa(inputs.backoffice_user_email.value + ":" + inputs.backoffice_user_password.value)
                }
              };
            inputNames:
              - backoffice_user_email
              - backoffice_user_password
            outputNames:
              - auth_token
          outgoingConnections:
            - auth_token->"Headers Auth" CWQvxx8D1qcVh_XhebpCb/auth_token
          visualData: -853.0324794349242/322.6224741102993/230/null//
        '[emED-dpUXaOWv-Y7v6KIE]:getGlobal "Global - Rivet Graph Name"':
          data:
            dataType: any
            id: rivet_graph_name
            onDemand: false
            useIdInput: false
            wait: true
          outgoingConnections:
            - value->"Headers - Create Prompt"
              1sbmsaD5JjVzc83rEEzC5/rivet_graph_name
          visualData: 901.2850449382717/2871.137837037037/230/322//
        '[fywYLAj_waj20lE-PRaJk]:compare "Compare"':
          data:
            comparisonFunction: ==
          outgoingConnections:
            - output->"Model Engine" hnvR2MiDSp7M5djmO6GRY/if
          visualData: 190/1860/190/297//
        '[hnvR2MiDSp7M5djmO6GRY]:ifElse "Model Engine"':
          data:
            unconnectedControlFlowExcluded: true
          outgoingConnections:
            - output->"Headers - Create Prompt" 1sbmsaD5JjVzc83rEEzC5/engine
          visualData: 519/1866/205/296//
        '[if3ZN-so-aM4f5QE8ntyD]:text "anthropic"':
          data:
            text: anthropic
          outgoingConnections:
            - output->"Model Engine" hnvR2MiDSp7M5djmO6GRY/true
          visualData: 17/2256/330/306//
        '[m190e98AhL7wN5UJIIFkE]:extractObjectPath "Extract Object Path"':
          data:
            path: $.Authorization
            usePathInput: false
          outgoingConnections:
            - match->"Headers - Create Prompt"
              1sbmsaD5JjVzc83rEEzC5/authorization
          visualData: -563/1139/280/null//
        '[rgY_KTkClMjjS0au5zdCe]:getGlobal "Global - System Role"':
          data:
            dataType: string
            id: system_role
            onDemand: false
            useIdInput: false
            wait: true
          outgoingConnections:
            - value->"Headers - Create Prompt" 1sbmsaD5JjVzc83rEEzC5/system_role
          visualData: 896/2607/230/291//
        '[rpCkWI3jJNa3OPvIy4_8d]:object "Get Template Id by Name Payload"':
          data:
            jsonTemplate: |-
              {
                "template_name": "{{template_name}}"
              }
          outgoingConnections:
            - output->"API CALL- GET TEMPLATEID BY NAME"
              OB-3rHEDcVGO96UtMqecU/req_body
          visualData: 178/882/230/309//
        '[vBSIWsbnQUd5OzR7NzzHu]:getGlobal "Get Global"':
          data:
            dataType: string
            id: rivet_project_file_content
            onDemand: false
            useIdInput: false
            wait: true
          outgoingConnections:
            - value->"If" w-3MQdvsDtSC7lvVfpS-G/value
          visualData: -940.4236681336458/1436.0089171420632/230/null//
        '[w-3MQdvsDtSC7lvVfpS-G]:if "If"':
          data:
            unconnectedControlFlowExcluded: true
          outgoingConnections:
            - output->"Upload Rivet Project to S3 Payload"
              VFRDxpzz3DI2q-26gGQZe/rivet_project_file_content
          visualData: -23.70805465020578/1465.7849810699588/155/318//
        '[xoF9FGaSpU79K4oAK835D]:ifElse "If/Else"':
          data:
            unconnectedControlFlowExcluded: true
          outgoingConnections:
            - output->"Graph Output" PWLOFYAvrMGNAbK5iGQSB/value
          visualData: 2814.139541069959/1310.5093069958848/205/313//
        '[zsBFtbd9Xwr5YNUsuBOnJ]:if "If"':
          data:
            unconnectedControlFlowExcluded: true
          outgoingConnections:
            - output->"Get Template Id by Name Payload"
              rpCkWI3jJNa3OPvIy4_8d/template_name
          visualData: -536/962/155/311//
        '[zzTIoTTmRBUaaFT-5wAFp]:object "Object"':
          data:
            jsonTemplate: |-
              {
                "message": "Deploy not run because deploy is marked as False"
              }
          outgoingConnections:
            - output->"If/Else" xoF9FGaSpU79K4oAK835D/false
          visualData: 2449.5975637860065/986.0117135802469/230/314//
    blT81coO59bxyTo9Vdut5:
      metadata:
        description: ""
        id: blT81coO59bxyTo9Vdut5
        name: ChatWithFunctions v2
      nodes:
        '[1sBJZFc6h8HN5zks9PsjP]:text "Text"':
          data:
            text: "{{input}}"
          visualData: -258.70874882222324/-1755.1965208751526/330/317//
        '[2CsKi4TB9hIzawnyoqFEa]:graphOutput "Graph Output"':
          data:
            dataType: object
            id: functions_response
          isSplitRun: true
          splitRunMax: 10
          visualData: 973.1583243663404/-1232.046930028492/330/304//
        '[2SQBKURczh60hJ3CsPzXP]:ifElse "If/Else"':
          data:
            unconnectedControlFlowExcluded: true
          outgoingConnections:
            - output->"Chat (Anthropic)" bOK5FNpVBNDPj_BKFja91/maxTokens
            - output->"Chat" qg26eTTGkVyYO5j_8GA6t/maxTokens
            - output->"Subgraph" 64_kuxyIQCSN01drwfECE/max_tokens
            - output->"Subgraph" zcAqNUSgZcSk_GmUUpXjT/max_tokens
          visualData: -900.8762117766144/-2402.029821916078/205/351//
        '[64_kuxyIQCSN01drwfECE]:subGraph "Subgraph"':
          data:
            graphId: No4XBpiQKMTWMDJ-0Cwpy
            useAsGraphPartialOutput: false
            useErrorOutput: false
          outgoingConnections:
            - prompt_text_with_entry->"Code" kgULFZxlKZKq-Gg0_NRhZ/raw_prompt
          visualData: -1971.1455358784149/-1573.716433729558/330/320//
        '[8YDd5uaKoLo09IHUwVnr7]:getGlobal "Global - Model Name"':
          data:
            dataType: string
            id: model_name
            onDemand: false
            useIdInput: false
            wait: true
          outgoingConnections:
            - value->"If/Else" qZBy4Vq_oltDZYxiNcxGk/false
          visualData: -1414.725367240582/-1477.89880591378/230/329//
        '[8q5cv8FdeBCWeYOxbHZMh]:graphInput "Graph Input"':
          data:
            dataType: string
            id: model_name
            useDefaultValueInput: false
          outgoingConnections:
            - data->"If/Else" qZBy4Vq_oltDZYxiNcxGk/if
            - data->"If/Else" qZBy4Vq_oltDZYxiNcxGk/true
          visualData: -1515.392362934404/-1201.4050084500354/330/338//
        '[BTappAsdV52_NqKSZrBOp]:code "Code"':
          data:
            code: >-
              const llmResultString = JSON.stringify(inputs.llm_result.value)
                
              let startIndex = llmResultString.indexOf("{");

              let endIndex = llmResultString.lastIndexOf("}");

              let parsedJson = {};

              let entrouNoIf = false;

              let entrouNoException = false;


              if (startIndex != -1 && endIndex != -1) {
                entrouNoIf = true;
                let jsonStr = llmResultString.slice(startIndex, endIndex + 1);
                // Attempt to parse the JSON
                jsonStr = jsonStr.replaceAll('\\n', '\n'); // replace occurrences of \n
                jsonStr = jsonStr.replaceAll('\\\\n', '\n');
                jsonStr = jsonStr.replaceAll('\\"', '"'); 
                jsonStr = jsonStr.replaceAll('\\\\"', '');
                try {
                  parsedJson = JSON.parse(jsonStr);
                } catch (error) {
                  entrouNoException = true;
                  parsedJson = {};
                }
                return {
                  output: {
                    value: parsedJson,
                    type: 'object'
                  },
                  llmResultString: {
                    value: llmResultString,
                    type: 'string'
                  },
                  jsonStr: {
                    value: jsonStr,
                    type: "string"
                  },
                  startIndex: {
                    value: startIndex,
                    type: "number"
                  },
                  endIndex: {
                    value: endIndex,
                    type: "number"
                  },
                  entrouNoIf: {
                    value: entrouNoIf,
                    type: "boolean"
                  },
                  entrouNoException: {
                    value: entrouNoException,
                    type: "boolean"
                  }
                }
              }

              else {
                throw new Error("No valid JSON found in the input string.");
              }
            inputNames:
              - llm_result
            outputNames:
              - output
              - llmResultString
              - jsonStr
              - startIndex
              - endIndex
              - entrouNoIf
              - entrouNoException
          outgoingConnections:
            - output->"If/Else" QoiUQKa2a9J4EE0lTpFJR/false
          visualData: 20.977412287131678/-1161.5970610124834/303.70337653572324/358//
        '[Ee1HzlpljImKG9lME3ajZ]:extractObjectPath "Extract Object Path"':
          data:
            path: $.arguments
            usePathInput: false
          isSplitRun: true
          outgoingConnections:
            - match->"If/Else" QoiUQKa2a9J4EE0lTpFJR/true
            - match->"Text" il6HUVXrfKeTr35GqDCJx/input
          splitRunMax: 10
          visualData: -54.59604244572697/-1409.225638175518/280/278//
        '[F4L-mrQAwfpk_719CqD8c]:graphInput "Graph Input"':
          data:
            dataType: string
            id: input
            useDefaultValueInput: true
          outgoingConnections:
            - data->"Code" XkkuLiJD1UMAbSh2SROF-/raw_prompt
            - data->"Subgraph" 64_kuxyIQCSN01drwfECE/promp_text
            - data->"Subgraph" zcAqNUSgZcSk_GmUUpXjT/promp_text
            - data->"Subgraph" zcAqNUSgZcSk_GmUUpXjT/prompt_text
          visualData: -2616.779078135069/-1122.3254871991237/330/323//
        '[GGM--0tvxHFs3p8DVwkTJ]:graphInput "Graph Input"':
          data:
            dataType: string
            id: openai_functions_description
            useDefaultValueInput: false
          outgoingConnections:
            - data->"GPT Function" YeQhJxqYGXyAK57aRHQd8/description
          visualData: -2157.3695107008743/-293.84888024135785/330/287//
        '[GQ0IfHDyQgovI1kww1qNp]:graphInput "Graph Input"':
          data:
            dataType: string
            editor: string
            id: max_tokens
            useDefaultValueInput: false
          outgoingConnections:
            - data->"If/Else" 2SQBKURczh60hJ3CsPzXP/if
            - data->"If/Else" 2SQBKURczh60hJ3CsPzXP/true
          visualData: -1413.8807465662464/-2150.367219943806/330/352//
        '[KhmeNb04d00Xxx_OqgcUG]:graphInput "Graph Input"':
          data:
            dataType: string
            id: entry
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Subgraph" 64_kuxyIQCSN01drwfECE/entry_text
            - data->"Subgraph" zcAqNUSgZcSk_GmUUpXjT/entry_text
          visualData: -2620.114638003822/-1397.3201208779965/330/310//
        '[QoiUQKa2a9J4EE0lTpFJR]:ifElse "If/Else"':
          data:
            unconnectedControlFlowExcluded: true
          isSplitRun: true
          outgoingConnections:
            - output->"Graph Output" 2CsKi4TB9hIzawnyoqFEa/value
          splitRunMax: 10
          visualData: 513.3080240294961/-1201.1405593302807/205/357//
        '[VX1TpB2iOESUldFmtDx7p]:getGlobal "Global - System Role"':
          data:
            dataType: string
            id: system_role
            onDemand: false
            useIdInput: false
            wait: true
          outgoingConnections:
            - value->"Chat" qg26eTTGkVyYO5j_8GA6t/systemPrompt
          visualData: -2043.5641973221739/-2777.3634972224672/230/346//
        '[XkkuLiJD1UMAbSh2SROF-]:code "Code"':
          data:
            code: >-
              // This is a code node, you can write and JS in here and it will
              be executed.

              // Inputs are accessible via an object `inputs` and data is typed (i.e. inputs.foo.type, inputs.foo.value)

              // Return an object with named outputs that match the output names specified in the node's config.

              // Output values must by typed as well (e.g. { bar: { type: 'string', value: 'bar' } }

              let raw_prompt = inputs.raw_prompt.value


              let functionsJson = {}

              functionsJson["type"] = "object"

              functionsJson["properties"] = {}

              functionsJson["required"] = []

              if (raw_prompt.includes(">>list_template<<")) {
                  functionsJson["properties"]["desired_items_list"] = {
                      "type": "array",
                      "items": {
                          "type": "object",
                          "required": [],
                          "properties": {}
                      }
                  }
                  functionsJson["required"].push("desired_items_list")
                  let listTemplateLines = raw_prompt.split("\n").filter(l => l.includes(">>list_template<<"))
                  for (let idx = 0; idx < listTemplateLines.length; idx++) {
                      let templateName = listTemplateLines[idx].match(/"([^']+)"/)[1]
                      functionsJson["properties"]["desired_items_list"]["items"]["required"].push(templateName)
                      functionsJson["properties"]["desired_items_list"]["items"]["properties"][templateName] = {
                          "type": "string"
                      }
                  }
              }

              else if (raw_prompt.includes(">>template<<") || raw_prompt.includes(">>template_no_validation<<")) {
                  let templateLines = raw_prompt.split("\n").filter(l => l.includes(">>template<<") || l.includes(">>template_no_validation<<"))
                  for (let idx = 0; idx < templateLines.length; idx++) {
                      let templateName = templateLines[idx].match(/"([^']+)"/)[1]
                      functionsJson["properties"][templateName] = {
                          "type": "string"
                      }
                      functionsJson["required"].push(templateName)
                  }
              }




              return {
                openai_functions: {
                  type: "object",
                  value: functionsJson
                }
              };
            inputNames:
              - raw_prompt
            outputNames:
              - openai_functions
          isSplitRun: true
          outgoingConnections:
            - openai_functions->"Calculate function tokens"
              hBnTOuzxCj30U3_mM_9ej/function_object
            - openai_functions->"GPT Function" YeQhJxqYGXyAK57aRHQd8/schema
          splitRunMax: 10
          visualData: -2021.3668227207286/-910.0185862334662/230/353//
        '[Y2R_ksy-QN-6vBpIF5GQz]:if "If"':
          data:
            unconnectedControlFlowExcluded: true
          isSplitRun: true
          outgoingConnections:
            - falseOutput->"Chat (Anthropic)" bOK5FNpVBNDPj_BKFja91/model
            - output->"Chat" qg26eTTGkVyYO5j_8GA6t/model
          splitRunMax: 10
          visualData: -762.9466716133595/-1052.3374657772915/155/331//
        '[YeQhJxqYGXyAK57aRHQd8]:gptFunction "GPT Function"':
          data:
            description: Get information from the bylaws as defined in the functions schema
            name: get_json
            schema: |-
              {
                "type": "object",
                "properties": {}
              }
            useDescriptionInput: true
            useNameInput: false
            useSchemaInput: true
          isSplitRun: true
          outgoingConnections:
            - function->"Chat" qg26eTTGkVyYO5j_8GA6t/functions
          splitRunMax: 10
          visualData: -1432.8560067110122/-404.5946774913552/280/297//
        '[bOK5FNpVBNDPj_BKFja91]:chatAnthropic "Chat (Anthropic)"':
          data:
            cache: false
            maxTokens: 1024
            model: claude-2
            stop: ""
            temperature: 0.5
            top_p: 1
            useAsGraphPartialOutput: true
            useMaxTokensInput: true
            useModelInput: true
            useStop: false
            useStopInput: false
            useTemperatureInput: true
            useTopKInput: false
            useTopP: false
            useTopPInput: true
            useUseTopPInput: false
          isSplitRun: true
          outgoingConnections:
            - response->"Code" BTappAsdV52_NqKSZrBOp/llm_result
          splitRunMax: 10
          visualData: -426.4938792202811/-984.8836212209395/305/266//
        '[hBnTOuzxCj30U3_mM_9ej]:code "Calculate function tokens"':
          data:
            code: >-
              // This is a code node, you can write and JS in here and it will
              be executed.

              // Inputs are accessible via an object `inputs` and data is typed (i.e. inputs.foo.type, inputs.foo.value)

              // Return an object with named outputs that match the output names specified in the node's config.

              // Output values must by typed as well (e.g. { bar: { type: 'string', value: 'bar' } }

              let function_object = inputs.function_object.value


              // Convert object to string

              const jsonString = JSON.stringify(function_object);


              // Calculate character length

              const characterLength = parseInt(jsonString.length / 4);


              return {
                output1: {
                  type: 'number',
                  value: characterLength
                }
              };
            inputNames:
              - function_object
            outputNames: output1
          visualData: -1508.2640739753049/-935.5523869961742/255.93308533375625/296//
        '[il6HUVXrfKeTr35GqDCJx]:text "Text"':
          data:
            text: "{{input}}"
          visualData: 279.8800333406663/-1636.8126820097466/414.1824091523258/316//
        '[kgULFZxlKZKq-Gg0_NRhZ]:code "Code"':
          data:
            code: >-
              // This is a code node, you can write and JS in here and it will
              be executed.

              // Inputs are accessible via an object `inputs` and data is typed (i.e. inputs.foo.type, inputs.foo.value)

              // Return an object with named outputs that match the output names specified in the node's config.

              // Output values must by typed as well (e.g. { bar: { type: 'string', value: 'bar' } }

              return {
                final_prompt: {
                  type: 'string',
                  value: inputs.raw_prompt.value.replace(">>template<<", "").replace(">>list_template<<", "").replace(">>explanation<<","").replace(":string","").replace(":dependency","").replace(":slashed_date","").replace(":currency","").replace(":quantity","").replace(":cpf","").replace(":cnpj","").replace(":no_validation","")
                }
              };
            inputNames:
              - raw_prompt
            outputNames:
              - final_prompt
          outgoingConnections:
            - final_prompt->"Chat (Anthropic)" bOK5FNpVBNDPj_BKFja91/prompt
            - final_prompt->"Chat" qg26eTTGkVyYO5j_8GA6t/prompt
          visualData: -1795.7049791910379/-1223.5209311381896/230/270//
        '[nH3rRNO8uqD48IywkKPHA]:getGlobal "Global - Temperature"':
          data:
            dataType: any
            id: temperature
            onDemand: false
            useIdInput: false
            wait: true
          outgoingConnections:
            - value->"Chat (Anthropic)" bOK5FNpVBNDPj_BKFja91/temperature
            - value->"Chat" qg26eTTGkVyYO5j_8GA6t/temperature
          visualData: -1336.426083281072/-2784.792055219752/230/347//
        '[nHl2Pu-mWfkTwm2E5aYee]:getGlobal "Global - Max Tokens"':
          data:
            dataType: any
            id: max_tokens
            onDemand: false
            useIdInput: false
            wait: true
          outgoingConnections:
            - value->"If/Else" 2SQBKURczh60hJ3CsPzXP/false
          visualData: -1332.6362708331974/-2474.18763621939/230/349//
        '[qZBy4Vq_oltDZYxiNcxGk]:ifElse "If/Else"':
          data:
            unconnectedControlFlowExcluded: true
          outgoingConnections:
            - output->"Subgraph" zcAqNUSgZcSk_GmUUpXjT/model
          visualData: -1042.8397308378958/-1321.1712108813192/205/337//
        '[qg26eTTGkVyYO5j_8GA6t]:chat "Chat"':
          data:
            cache: false
            code: ""
            enableFunctionUse: true
            frequencyPenalty: 0
            headers: []
            maxTokens: 1024
            model: gpt-3.5-turbo-16k-0613
            overrideMaxTokens: 128000
            presencePenalty: 0
            responseFormat: ""
            stop: ""
            temperature: 0.5
            toolChoice: function
            toolChoiceFunction: get_json
            top_p: 1
            useAsGraphPartialOutput: true
            useFrequencyPenaltyInput: false
            useMaxTokensInput: true
            useModelInput: true
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: true
            useTopP: false
            useTopPInput: true
            useUseTopPInput: false
            useUserInput: false
          isSplitRun: true
          outgoingConnections:
            - function-call->"Extract Object Path" Ee1HzlpljImKG9lME3ajZ/object
            - response->"Text" 1sBJZFc6h8HN5zks9PsjP/input
          splitRunMax: 10
          visualData: -427.16693645686104/-1522.07998158314/230/277//
        '[yTFurinFtkt2IaehP-Rno]:code "Code"':
          data:
            code: >-
              // This is a code node, you can write and JS in here and it will
              be executed.

              // Inputs are accessible via an object `inputs` and data is typed (i.e. inputs.foo.type, inputs.foo.value)

              // Return an object with named outputs that match the output names specified in the node's config.

              // Output values must by typed as well (e.g. { bar: { type: 'string', value: 'bar' } }

              let is_open_ai = inputs.model_name.value.includes("gpt") ? true : false


              return {
                is_open_ai: {
                  type: 'boolean',
                  value: is_open_ai
                }
              };
            inputNames:
              - model_name
            outputNames:
              - is_open_ai
          isSplitRun: true
          outgoingConnections:
            - is_open_ai->"If" Y2R_ksy-QN-6vBpIF5GQz/if
            - is_open_ai->"If/Else" QoiUQKa2a9J4EE0lTpFJR/if
          splitRunMax: 10
          visualData: -1132.3565284747758/-1059.8063969075254/230/330//
        '[zcAqNUSgZcSk_GmUUpXjT]:subGraph "Subgraph"':
          data:
            graphId: iWFa6csewSy4no6oMwbzD
            useAsGraphPartialOutput: false
            useErrorOutput: false
          outgoingConnections:
            - model->"Code" yTFurinFtkt2IaehP-Rno/model_name
            - model->"If" Y2R_ksy-QN-6vBpIF5GQz/value
            - model->"Subgraph" 64_kuxyIQCSN01drwfECE/model
          visualData: -2205.9427681505094/-1939.5252991355615/330/309//
        '[zf7-ACnoTs55nIMQh-LA0]:getGlobal "Global - Top P"':
          data:
            dataType: any
            id: top_p
            onDemand: false
            useIdInput: false
            wait: true
          outgoingConnections:
            - value->"Chat (Anthropic)" bOK5FNpVBNDPj_BKFja91/top_p
            - value->"Chat" qg26eTTGkVyYO5j_8GA6t/top_p
          visualData: -2042.523492877107/-2476.0942642249074/230/350//
    iWFa6csewSy4no6oMwbzD:
      metadata:
        description: ""
        id: iWFa6csewSy4no6oMwbzD
        name: CheckModelChange
      nodes:
        '[1c4q8hLjOc1K2DiHyeLqt]:graphInput "Graph Input"':
          data:
            dataType: string
            id: entry_text
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Code" YJBQ7VwhEY_kg3nm0jyn3/entry_text
            - data->"Log Output" 86oIof4IpZ7GkxoS0WQMB/entry
            - data->"Subgraph" gSQqmgNaq5V-yiK8dOkHD/text
          visualData: 203.99074715579488/647.6939740861047/330/26//
        '[1nDEa2WWvh1VucPuNhBhx]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: model
          outgoingConnections:
            - valueOutput->"Log Output" 86oIof4IpZ7GkxoS0WQMB/log_text
            - valueOutput->"Log Output" 86oIof4IpZ7GkxoS0WQMB/model_name
          visualData: 1844.310966867128/388.1981766254852/330/34//
        '[3sj-nZ6B6xttUf-fuQBAT]:subGraph "Subgraph"':
          data:
            graphId: k0HWnuSssHo77oxIe13DD
            useAsGraphPartialOutput: false
            useErrorOutput: false
          outgoingConnections:
            - output->"Code" YJBQ7VwhEY_kg3nm0jyn3/prompt_token_size
          visualData: 754.7892237645166/269.96423731092835/330/31//
        '[86oIof4IpZ7GkxoS0WQMB]:subGraph "Log Output"':
          data:
            graphId: AyQeZ4we5T_SGin85M5XS
            useAsGraphPartialOutput: false
            useErrorOutput: false
          visualData: 2310/412/330/35//
        '[9ir98PprpwemZj4LDTBKG]:graphInput "Graph Input"':
          data:
            dataType: string
            id: model
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Code" YJBQ7VwhEY_kg3nm0jyn3/model
          visualData: 669.0738015148012/1024.7090088471368/330/30//
        '[STJsByCUqEVfcpZezC3JV]:graphInput "Graph Input"':
          data:
            dataType: string
            id: prompt_text
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Code" YJBQ7VwhEY_kg3nm0jyn3/prompt_text
            - data->"Log Output" 86oIof4IpZ7GkxoS0WQMB/prompt_text
            - data->"Subgraph" 3sj-nZ6B6xttUf-fuQBAT/text
          visualData: 288.8313945381036/342.9982573568215/330/24//
        '[TGmDJGKqR8HvjuFNrS6kS]:graphInput "Graph Input"':
          data:
            dataType: number
            id: max_tokens
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Code" YJBQ7VwhEY_kg3nm0jyn3/max_tokens
          visualData: 672.6145553413895/786.3459603133/330/16//
        '[YJBQ7VwhEY_kg3nm0jyn3]:code "Code"':
          data:
            code: >
              // This functions sums the prompt and entry text tokens quantity


              // INPUTS

              let prompt_text = inputs.prompt_text.value;

              let entry_text = inputs.entry_text.value;

              let prompt_token_size = inputs.prompt_token_size.value;

              let entry_token_size = inputs.entry_token_size.value;

              let max_tokens = inputs.max_tokens.value;

              let model = inputs.model.value;

              let force_model_change = inputs.force_model_change.value;


              // CONSTANTS

              const entry_pattern = /<<([^>]+)>>/;

              const correctionFactor = 0.8

              const modelsTokenLimitDict = {
                  "gpt-4-1106-preview": parseInt(128000 * correctionFactor),
                  "gpt-4": parseInt(8192 * correctionFactor),
                  "claude-2": parseInt(100000 * correctionFactor),
                  "gpt-3.5-turbo-16k": parseInt(16385 * correctionFactor),
              };


              let total_token_sum = prompt_token_size + entry_token_size + max_tokens


              if (force_model_change == "false" && (
                  (model === "gpt-4" && total_token_sum > modelsTokenLimitDict["gpt-4"]) ||
                  (model === "gpt-4-1106-preview" && total_token_sum > modelsTokenLimitDict["gpt-4-1106-preview"]) ||
                  (model === "gpt-3.5-turbo-16k" && total_token_sum > modelsTokenLimitDict["gpt-3.5-turbo-16k"])
              )) {
                  model = "claude-2";
              }



              prompt_with_replace = prompt_text.replace(entry_pattern, entry_text)


              return {
                  model: {
                    type: 'string',
                    value: model
                  },
                  token_limit: {
                    type: 'number',
                    value: modelsTokenLimitDict[model]
                  }
                };
            inputNames:
              - prompt_text
              - entry_text
              - prompt_token_size
              - entry_token_size
              - max_tokens
              - model
              - force_model_change
            outputNames:
              - model
              - token_limit
          outgoingConnections:
            - model->"Graph Output" 1nDEa2WWvh1VucPuNhBhx/value
          visualData: 1232.0725210575772/330.23097789782946/393.22101351278/28//
        '[gSQqmgNaq5V-yiK8dOkHD]:subGraph "Subgraph"':
          data:
            graphId: k0HWnuSssHo77oxIe13DD
            useAsGraphPartialOutput: false
            useErrorOutput: false
          outgoingConnections:
            - output->"Code" YJBQ7VwhEY_kg3nm0jyn3/entry_token_size
          visualData: 613.1728698194144/560.9120269363775/330/25//
        '[neUn7TJQUPbKwZCwtdokL]:getGlobal "Get Global"':
          data:
            code: ""
            dataType: boolean
            id: force_model_change
            onDemand: false
            useIdInput: false
            wait: true
          outgoingConnections:
            - value->"Code" YJBQ7VwhEY_kg3nm0jyn3/force_model_change
          visualData: 835.6080735204889/1204.8932886117398/230/33//
    k0HWnuSssHo77oxIe13DD:
      metadata:
        description: ""
        id: k0HWnuSssHo77oxIe13DD
        name: getTextTokenSize
      nodes:
        '[Ai5zUVcMWDPMviXnP9idx]:prompt "Prompt"':
          data:
            computeTokenCount: true
            enableFunctionCall: false
            promptText: "{{input}}"
            type: user
            useTypeInput: false
          outgoingConnections:
            - tokenCount->"Graph Output" l6N_FTumGBffUnY_YWusx/value
          visualData: 901.5877453896491/349.64961332540156/280/7//
        '[ar5CDcmt7BY-ar27mWdca]:graphInput "Graph Input"':
          data:
            dataType: string
            id: text
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Prompt" Ai5zUVcMWDPMviXnP9idx/input
          visualData: 439/325/330/2//
        '[l6N_FTumGBffUnY_YWusx]:graphOutput "Graph Output"':
          data:
            dataType: number
            id: output
          visualData: 1370.8322427126711/360.1195716835217/330/4//
    oy4gSk9dfV4Rlc86I22Tx:
      metadata:
        description: ""
        id: oy4gSk9dfV4Rlc86I22Tx
        name: main_challenge
      nodes:
        '[0RCJr1o0w91eR-XvvayXl]:coalesce "Coalesce"':
          visualData: -2509.3540436136223/1504.3307445482972/180/661//
        '[4XDRGesJQWWWjM0DLQ1iz]:context "MAX_TOKENS"':
          data:
            dataType: any
            defaultValue: "4000"
            id: max_tokens
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Set Global" QQ6ygz6K9Drx0l62PKb2v/value
          visualData: -2684.8469975259177/2625.7698077199657/330/629//
        '[5tKXwYZp-NUwjbWB9Hsp3]:setGlobal "Set Global"':
          data:
            dataType: string
            id: rivet_project_file_content
            useIdInput: false
          visualData: -2270.704193837281/1469.7132543312428/230/662//
        '[6ZQ4E2nJGcYDksnCYemn9]:setGlobal "Set Global"':
          data:
            dataType: string
            id: backoffice_user_password
            useIdInput: false
          visualData: -3282/1451/230/null//
        '[AFzw8dGBjFW73qErnKQQ8]:context "Context"':
          data:
            dataType: boolean
            defaultValue: "false"
            id: is_running_on_local_rivet_app
            useDefaultValueInput: true
          outgoingConnections:
            - data->"Compare" vwBcmWGXNmeQCPUHIsnKu/b
            - data->"Set Global" HB7nZ5M6aGwKGnjbhS4Qn/value
          visualData: -4263.457777777778/768.1244444444444/330/568//
        '[C-5IJqdXlimvzFC5OUYU9]:setGlobal "Set Global"':
          data:
            dataType: string
            id: rivet_project_file_name
            useIdInput: false
          visualData: -2399.8482760805014/1238.5225739573727/230/443//
        '[D_f_pgtRI4BMWhO9TNWt3]:context "SYSTEM_ROLE"':
          data:
            dataType: string
            defaultValue: You are a trained lawyer in Brazil. Your job is to read company
              bylaws and return a JSON with the key pieces of a legal document.
            id: system_role
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Set Global" TMqjuvQ7OlKv88DMgNaX-/value
          visualData: -2760.1117799377585/1777.466308742296/330/354//
        '[Djl5y_ISwceJQFeKJD-86]:context "MODEL_NAME"':
          data:
            dataType: string
            defaultValue: gpt-4-turbo-preview
            id: model_name
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Set Global" z4uxTnkfSHVgimAkaDwL0/value
          visualData: -2736.0186766414827/1973.0803668304561/330/355//
        '[EKapXzVd7FOs2PSRoFpBB]:subGraph "Subgraph"':
          data:
            graphId: wMCzVWaRyu2cpdzhHyNSg
            useAsGraphPartialOutput: false
            useErrorOutput: false
          outgoingConnections:
            - output->"Text" f-9c9-71WAgHlRvkw7Dfq/input
          visualData: -944.107100700182/1044.4618916857917/330/689//
        '[F79MtZQa56ueWbNskQsek]:text "Text"':
          data:
            text: >-
              # Macro Research


              progressos nas medidas de receitas tributárias nas últimas semanas (ver seção sobre “Contas Públicas” acima) – embora a carga fiscal mais elevada também possa pressionar os preços ao consumidor no curto prazo.


              Rumo ao neutro, a 9,00%. Nessas circunstâncias, reduzimos nossa previsão de inflação em 2024, para 3,7% (4,1% antes), e vemos agora uma taxa de câmbio mais apreciada (4,70 reais por dólar vs 4,85 antes). Isso abre espaço para o Banco Central estender o ciclo de flexibilização monetária, atingindo o nível neutro mais cedo: esperamos que a taxa Selic atinja 9,00% já em 2024 – 1,0 p.p. abaixo do que prevíamos no último relatório mensal. Para 2025, vemos a taxa básica estável em 9,00%, mantendo a projeção anterior.


              # Projeções XP


              | |2019|2020|2021|2022|2023 (P)|2024 (P)|2025 (P)|

              |---|---|---|---|---|---|---|---|

              |Crescimento do PIB (var. real %)|1,2|-3,3|4,8|3,0|3,0|1,5|2,0|

              |Taxa de desemprego (%, dessaz., fim de período)|11,5|14,7|11,7|8,3|7,8|8,3|8,0|

              |IPCA (var. 12m %)|4,3|4,5|10,1|5,8|4,6|3,7|4,0|

              |SELIC (% a.a, fim de período)|4,50|2,00|9,25|13,75|11,75|9,00|9,00|

              |Taxa de Câmbio (R$/US$, fim de período)|4,03|5,20|5,58|5,28|4,86|4,70|4,90|

              |Resultado primário do governo central (% PIB)|-1,2|-9,8|-0,4|0,5|-2,4|-0,6|-1,1|

              |Resultado primário do setor público (% PIB)|-0,8|-9,2|0,7|1,2|-2,3|-0,6|-1,0|

              |Dívida bruta - DBGG (% PIB)|74,4|86,9|77,3|71,7|75,2|78,0|80,6|

              |Balança Comercial (US$ Bi) - MDIC|35,2|50,4|61,4|62,14|98,8|86,0|88,5|

              |Exportações (US$ Bi) - MDIC|221,1|209,2|280,8|334,8|337,3|342,0|355,0|

              |Importações (US$ Bi) - MDIC|185,9|158,8|219,4|272,6|238,5|256,0|266,5|

              |Conta Corrente (US$ Bi)|-68,0|-28,2|-46,4|-53,6|-27,1|-32,0|-42,5|

              |Conta Corrente (% PIB)|-3,6|-1,9|-2,8|-2,8|-1,2|-1,3|-1,7|

              |IDP (US$ Bi)|69,2|37,8|46,4|87,2|58,0|67,0|75,0|

              |IDP (% PIB)|3,7|2,6|2,8|4,5|2,7|2,8|3,0|

              |PIB Nominal (US$ Bi)|1.873|1.475|1.670|1.951|2.179|2.400|2.520|

              |PIB Nominal (R$ Bi)|7.389|7.610|9.012|10.080|10.808|11.404|12.136|


              Fonte: IBGE, BCB, Bloomberg, XP
          outgoingConnections:
            - output->"Text" qBM1H6PBiiDklUaPYJS56/input
          visualData: -1852.687404817875/790.5633033693789/330/688//
        '[HB7nZ5M6aGwKGnjbhS4Qn]:setGlobal "Set Global"':
          data:
            dataType: boolean
            id: is_running_on_local_rivet_app
            useIdInput: false
          visualData: -3723.602962962963/918.045925925926/230/569//
        '[HT7Z9BfBiIz6suEAFN2wS]:setGlobal "Set Global"':
          data:
            dataType: any
            id: temperature
            useIdInput: false
          visualData: -2277.6176898777703/2256.173274733328/230/367//
        '[HdKteeoaMPwBqfw1mPw1M]:context "FORCE_MODEL_CHANGE"':
          data:
            dataType: string
            defaultValue: "false"
            id: force_model_change
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Set Global" ySq9gxW0JB9Nn3XtvXM6U/value
          visualData: -2683.8539085490575/2933.1346886232377/355.64976463421/633//
        '[HzQUlRztn2wi1xsExdSSK]:context "TOP_P"':
          data:
            dataType: any
            defaultValue: "0.5"
            id: top_p
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Set Global" OL7vv5oAz4giHgBY7pM9j/value
          visualData: -2693.306972225292/2344.993124625841/330/357//
        '[InH7qadl8x2Dmhe3bK9dQ]:setGlobal "SET GLOBAL"':
          data:
            dataType: string
            id: backoffice_user_email
            text: ""
            useIdInput: false
          visualData: -3285/1220/230/483//
        '[KWBqHYD_RirEA5LonZWuF]:boolean "Bool"':
          data:
            value: true
          outgoingConnections:
            - value->"Context" AFzw8dGBjFW73qErnKQQ8/default
          visualData: -4562/817/160/null//
        '[KxMx4BbCfvO-4zhZJOz8V]:setGlobal "Set Global"':
          data:
            dataType: string
            id: rivet_graph_name
            useIdInput: false
          visualData: -3246.6666666666665/1723.111111111111/230/489//
        '[LI0cd76sb4KJEKHWmmRXn]:context "TEMPERATURE"':
          data:
            dataType: any
            defaultValue: "0.2"
            id: temperature
            promptText: ""
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Set Global" HT7Z9BfBiIz6suEAFN2wS/value
          visualData: -2711.8114664086957/2159.7349187457767/330/356//
        '[M4WvDO4iH44_YEOPKOrxo]:text "BACKOFFICE USER EMAIL"':
          data:
            text: mateus@talismanai.co
          visualData: -3740/1214/330/572//
        '[NUXA2VIkC8h5AFzp-FC1F]:setGlobal "Set Global"':
          data:
            dataType: string
            id: template_name
            useIdInput: false
          visualData: -2480.4686470226147/764.8182721033512/230/660//
        '[OL7vv5oAz4giHgBY7pM9j]:setGlobal "Set Global"':
          data:
            dataType: any
            id: top_p
            useIdInput: false
          visualData: -2247.0095905779726/2487.2663780296043/230/368//
        '[QFAxw7FWMRWPCws8VFkm_]:readFile "Rivet Project File Content"':
          data:
            asBinary: false
            errorOnMissingFile: false
            path: /Users/mateuscostaribeiro/Desktop/talisman_challenge.rivet-project
            usePathInput: false
          outgoingConnections:
            - content->"Coalesce" 0RCJr1o0w91eR-XvvayXl/input1
          visualData: -2876.8888888888887/1455.4444444444443/280/552//
        '[QQ6ygz6K9Drx0l62PKb2v]:setGlobal "Set Global"':
          data:
            dataType: any
            id: max_tokens
            useIdInput: false
          visualData: -2233.722808447473/2749.9375886186344/230/369//
        '[TMqjuvQ7OlKv88DMgNaX-]:setGlobal "Set Global"':
          data:
            dataType: string
            id: system_role
            useIdInput: false
          visualData: -2320.329394293961/1742.1232766786695/230/539//
        '[V3HKEi5SxAXIo2QQpjfyn]:array "Array"':
          data:
            flatten: false
            flattenDeep: false
          outgoingConnections:
            - output->"External Call" sZR_L2QXhbt5Q7Gms7HsM/arguments
          visualData: 450.5606156487769/1110.5412739280744/230/709//
        '[XBxC_Ebc7vBVYk3K__yow]:object "Object"':
          data:
            jsonTemplate: |-
              {
                "date": "24-03",
                "csv": "{{input}}"
              }
          outgoingConnections:
            - output->"Array" V3HKEi5SxAXIo2QQpjfyn/input3
          visualData: -72.93190226076689/1188.3920321375854/230/702//
        '[a2i5eWeY6XVxbUBMm2IG-]:text "Template Name"':
          data:
            text: Main challenge
          visualData: -2899/780/330/639//
        '[atBfCpfy1sxk1IJvd6T9M]:object "Object"':
          data:
            jsonTemplate: |-
              {
                "date": "24-01",
                "csv": "{{input}}"
              }
          outgoingConnections:
            - output->"Array" V3HKEi5SxAXIo2QQpjfyn/input1
          visualData: -66.89160397719522/752.1502424361554/230/700//
        '[czHAWQxaQpYRZ_7tcZ6fO]:text "Default Rivet Graph Name"':
          data:
            text: main_challenge
          visualData: -3748.6666666666665/1758/330/621//
        '[f-9c9-71WAgHlRvkw7Dfq]:text "Text"':
          data:
            text: "{{input}}"
          outgoingConnections:
            - output->"Object" XBxC_Ebc7vBVYk3K__yow/input
            - output->"Object" atBfCpfy1sxk1IJvd6T9M/input
            - output->"Object" k8dIELrjOx4t-flKbMVpJ/input
            - output->"Object" rzzRLJVyqGddroEz3qv4D/input
          visualData: -554.6848472561522/1119.7238720636944/330/691//
        '[iRVNT9OML9qnogd7ZVY9u]:subGraph "Deploy Template Name"':
          data:
            graphId: VBfwPp3tXdEmMiB6_sXzD
            useAsGraphPartialOutput: false
            useErrorOutput: false
          visualData: 2653/646/330/411//
        '[ihDp8JSnMxDY-SEFwq_UT]:getGlobal "Global - Deploy"':
          data:
            dataType: boolean
            id: deploy
            onDemand: false
            useIdInput: false
            wait: true
          outgoingConnections:
            - value->"Deploy Template Name" iRVNT9OML9qnogd7ZVY9u/deploy
          visualData: 2320.267940907117/595.8935897349443/230/541//
        '[k8dIELrjOx4t-flKbMVpJ]:object "Object"':
          data:
            jsonTemplate: |-
              {
                "date": "24-02",
                "csv": "{{input}}"
              }
          outgoingConnections:
            - output->"Array" V3HKEi5SxAXIo2QQpjfyn/input2
          visualData: -72.93190226076688/980.3383010928231/230/704//
        '[l3ZrYn61QVJMkSYJ_cagM]:text "Rivet Project File Name"':
          data:
            text: main_challenge
          visualData: -2874.2725925925924/1256.5792592592593/330/567//
        '[pHmEx3aXTf1LbragICZQ2]:text "BACKOFFICE USER PASSWORD"':
          data:
            text: 3Q75=866YjOt!
          visualData: -3751.597266811033/1468.4055459121698/330/665//
        '[qBM1H6PBiiDklUaPYJS56]:text "Text"':
          data:
            text: |-
              Pegue os dados do ano de 2024


              =========== input ============
              {{input}}
              ===============================
          outgoingConnections:
            - output->"Subgraph" EKapXzVd7FOs2PSRoFpBB/input
          visualData: -1377.1670490136898/932.6586696729094/330/687//
        '[rzzRLJVyqGddroEz3qv4D]:object "Object"':
          data:
            jsonTemplate: |-
              {
                "date": "24-04",
                "csv": "{{input}}"
              }
          outgoingConnections:
            - output->"Array" V3HKEi5SxAXIo2QQpjfyn/input4
          visualData: -62.86473845481411/1406.5129269883005/230/705//
        '[sZR_L2QXhbt5Q7Gms7HsM]:externalCall "External Call"':
          data:
            functionName: calculateDelta
            useErrorOutput: false
            useFunctionNameInput: false
          visualData: 902.2266884067851/1223.2935085547454/180/712//
        '[vwBcmWGXNmeQCPUHIsnKu]:compare "Compare"':
          data:
            comparisonFunction: and
          visualData: -2859/1004/190/528//
        '[ySq9gxW0JB9Nn3XtvXM6U]:setGlobal "Set Global"':
          data:
            dataType: string
            id: force_model_change
            useIdInput: false
          visualData: -2197.714752897587/3055.3148794289586/230/634//
        '[ypw23LIKC38AVJj5S05Vi]:boolean "Deploy"':
          data:
            promptText: ""
            value: false
          outgoingConnections:
            - value->"Compare" vwBcmWGXNmeQCPUHIsnKu/a
          visualData: -3372.9333333333334/652.9333333333334/160/638//
        '[z4uxTnkfSHVgimAkaDwL0]:setGlobal "Set Global"':
          data:
            dataType: string
            id: model_name
            useIdInput: false
          visualData: -2303.0080771246635/1991.3069856612458/230/540//
        '[zl0vrqtxhyCwz_2fuBtjh]:setGlobal "Set Global"':
          data:
            dataType: boolean
            id: deploy
            useIdInput: false
          visualData: -2400.8482760805014/1044.5225739573727/230/512//
    wMCzVWaRyu2cpdzhHyNSg:
      metadata:
        description: ""
        id: wMCzVWaRyu2cpdzhHyNSg
        name: mardownToCSV
      nodes:
        '[-VU2jS6518qJ7TKk8ljPM]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: output
          visualData: 1871.2905820690648/477.8277149377529/330/698//
        '[81-yDJjF_4zZMxntj-TRV]:graphInput "Graph Input"':
          data:
            dataType: string
            id: input
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Chat" IONoorY6mA3ArOlRtAPDS/prompt
          visualData: 382.9183890004573/671.2946586141006/330/685//
        '[9f4bkKbhHcmnuauJelp2w]:text "Text"':
          data:
            text: >-
              You are a financial analysis expert, your job is to get a markdown
              text, and extract the table from it. You are going to be given a
              year, and only get data from this year.


              Also, I only want to extract the rows: 

              * IPCA (should be outputed as ipca, lower case)

              * SELIC (should be outputed as selic, lower case)

              * Taxa de Câmbio (should be outputed as cambio, lower case)


              IMPORTANT! The final format should be:

              |---|---|

              |type|value|

              |ipca|{IPCA_VALUE}|

              |selic|{SELIC_VALUE}|

              |cambio|{CAMBIO_VALUE}|

              ------------------------

              example: Extract the data from the year of 2025

              =============input===============

              # Macro Research


              progressos nas medidas de receitas tributárias nas últimas semanas (ver seção sobre “Contas Públicas” acima) – embora a carga fiscal mais elevada também possa pressionar os preços ao consumidor no curto prazo.


              Rumo ao neutro, a 9,00%. Nessas circunstâncias, reduzimos nossa previsão de inflação em 2024, para 3,7% (4,1% antes), e vemos agora uma taxa de câmbio mais apreciada (4,70 reais por dólar vs 4,85 antes). Isso abre espaço para o Banco Central estender o ciclo de flexibilização monetária, atingindo o nível neutro mais cedo: esperamos que a taxa Selic atinja 9,00% já em 2024 – 1,0 p.p. abaixo do que prevíamos no último relatório mensal. Para 2025, vemos a taxa básica estável em 9,00%, mantendo a projeção anterior.


              # Projeções XP


              | |2019|2020|2021|2022|2023 (P)|2024 (P)|2025 (P)|

              |---|---|---|---|---|---|---|---|

              |Crescimento do PIB (var. real %)|1,2|-3,3|4,8|3,0|3,0|1,5|2,0|

              |Taxa de desemprego (%, dessaz., fim de período)|11,5|14,7|11,7|8,3|7,8|8,3|8,0|

              |IPCA (var. 12m %)|4,3|4,5|10,1|5,8|4,6|3,7|4,0|

              |SELIC (% a.a, fim de período)|4,50|2,00|9,25|13,75|11,75|9,00|9,00|

              |Taxa de Câmbio (R$/US$, fim de período)|4,03|5,20|5,58|5,28|4,86|4,70|4,90|

              |Resultado primário do governo central (% PIB)|-1,2|-9,8|-0,4|0,5|-2,4|-0,6|-1,1|

              |Resultado primário do setor público (% PIB)|-0,8|-9,2|0,7|1,2|-2,3|-0,6|-1,0|

              |Dívida bruta - DBGG (% PIB)|74,4|86,9|77,3|71,7|75,2|78,0|80,6|

              |Balança Comercial (US$ Bi) - MDIC|35,2|50,4|61,4|62,14|98,8|86,0|88,5|

              |Exportações (US$ Bi) - MDIC|221,1|209,2|280,8|334,8|337,3|342,0|355,0|

              |Importações (US$ Bi) - MDIC|185,9|158,8|219,4|272,6|238,5|256,0|266,5|

              |Conta Corrente (US$ Bi)|-68,0|-28,2|-46,4|-53,6|-27,1|-32,0|-42,5|

              |Conta Corrente (% PIB)|-3,6|-1,9|-2,8|-2,8|-1,2|-1,3|-1,7|

              |IDP (US$ Bi)|69,2|37,8|46,4|87,2|58,0|67,0|75,0|

              |IDP (% PIB)|3,7|2,6|2,8|4,5|2,7|2,8|3,0|

              |PIB Nominal (US$ Bi)|1.873|1.475|1.670|1.951|2.179|2.400|2.520|

              |PIB Nominal (R$ Bi)|7.389|7.610|9.012|10.080|10.808|11.404|12.136|


              Fonte: IBGE, BCB, Bloomberg, XP

              =================================


              =============output===============

              |type|value|

              |---|---|

              |ipca|4,0|

              |selic|9,00|

              |cambio|4,90|

              ==================================
          outgoingConnections:
            - output->"Chat" IONoorY6mA3ArOlRtAPDS/systemPrompt
          visualData: 376.14816126718665/275.6539951981982/330/703//
        '[BlmdzlhUt77Mtz0PbotY-]:chat "Chat"':
          data:
            additionalParameters: []
            cache: false
            enableFunctionUse: false
            headers: []
            maxTokens: 1024
            modalitiesIncludeAudio: false
            modalitiesIncludeText: false
            model: gpt-4o-mini
            outputUsage: false
            parallelFunctionCalling: true
            reasoningEffort: medium
            stop: ""
            temperature: 0.5
            top_p: 1
            useAdditionalParametersInput: false
            useAsGraphPartialOutput: true
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePredictedOutput: false
            usePresencePenaltyInput: false
            useReasoningEffortInput: false
            useServerTokenCalculation: true
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
            useUserInput: false
          outgoingConnections:
            - response->"Graph Output" -VU2jS6518qJ7TKk8ljPM/value
          visualData: 1421.7586637160443/426.8665828820714/230/696//
        '[IONoorY6mA3ArOlRtAPDS]:chat "Chat"':
          data:
            additionalParameters: []
            cache: false
            enableFunctionUse: false
            headers: []
            maxTokens: 1024
            modalitiesIncludeAudio: false
            modalitiesIncludeText: false
            model: gpt-4o-mini
            outputUsage: false
            parallelFunctionCalling: true
            reasoningEffort: medium
            stop: ""
            temperature: 0.2
            top_p: 1
            useAdditionalParametersInput: false
            useAsGraphPartialOutput: true
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePredictedOutput: false
            usePresencePenaltyInput: false
            useReasoningEffortInput: false
            useServerTokenCalculation: true
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
            useUserInput: false
          outgoingConnections:
            - response->"Chat" BlmdzlhUt77Mtz0PbotY-/prompt
          visualData: 840.5278996767638/518.2534514222897/485.0335231417839/702//
        '[SRe7WKev8-F6LLhp9QVE1]:text "Text"':
          data:
            text: >
              You are a tool that converts markdown tables to csv.


              Detail, the input uses coma (",") for decimal, you should use dot (".")


              You will receive tables in the format:

              ======= input format ========


              |type|value|

              |---|---|

              |ipca|{IPCA_VALUE}|

              |selic|{SELIC_VALUE}|

              |cambio|{CAMBIO_VALUE}|

              ------------------------

              ==============================


              and need to output in the following csv format:

              ========= output format =======

              type,value

              ipca,{IPCA_VALUE}

              selic,{SELIC_VALUE}

              cambio,{CAMBIO_VALUE}

              ===============================

              --------------------------------------------

              for example:


              ======== input ========

              |type|value|

              |---|---|

              |ipca|4,0|

              |selic|9,00|

              |cambio|4,90|

              ========================

              should be converted to

              ========= output =======

              type,value

              ipca,4.0

              selic,9.0

              cambio,4.9

              =========================
          outgoingConnections:
            - output->"Chat" BlmdzlhUt77Mtz0PbotY-/systemPrompt
          visualData: 841.6635010573694/-40.919928459836676/330/700//
    ziO3SDNmpuw8VcqPBf8aL:
      metadata:
        description: ""
        id: ziO3SDNmpuw8VcqPBf8aL
        name: pdfToMarkdown
      nodes:
        '[2Bl0RKsmye8UfAxaOVMob]:object "Object"':
          data:
            jsonTemplate: |-
              {
                "id": "{{input}}",
                "timeout": 10
              }
          outgoingConnections:
            - output->"External Call" dgw3tPvNLlIhMhlJE6nVh/arguments
          visualData: 2469.4042524351025/471.4117526041322/230/1308//
        '[QIcUQyBWc1SjrlM27VBVM]:externalCall "External Call"':
          data:
            functionName: uploadFile
            useErrorOutput: false
            useFunctionNameInput: false
          outgoingConnections:
            - result->"Extract Object Path" QpMo_rD3KJg56uPOvQelh/object
          visualData: 1406.3800753217217/455.1650563303846/501.0867700909696/1306//
        '[QpMo_rD3KJg56uPOvQelh]:extractObjectPath "Extract Object Path"':
          data:
            path: $.id
            usePathInput: false
          outgoingConnections:
            - match->"Object" 2Bl0RKsmye8UfAxaOVMob/input
          visualData: 2046.8121637988534/448.7270204291975/280/1307//
        '[Wiq5aIb48Hc4jwsc17x71]:graphInput "Graph Input"':
          data:
            dataType: string
            defaultValue: /Users/paulochade/Documents/projects/enter/riviet/data/table/table-24-01.pdf
            id: input
            text: ""
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Object" adynGrRiO1Kb_AvP4uoZg/value1
          visualData: 582/420/330/1298//
        '[ZAnhfZqcJ5fwUEPZ_uqLl]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: output
          visualData: 3080.052926309161/534.0333854768313/330/1310//
        '[adynGrRiO1Kb_AvP4uoZg]:object "Object"':
          data:
            jsonTemplate: '{"path": "{{value1}}"}'
          outgoingConnections:
            - output->"External Call" QIcUQyBWc1SjrlM27VBVM/arguments
          visualData: 1037.2889406026509/467.8684025333322/241.22036593095822/1305//
        '[dgw3tPvNLlIhMhlJE6nVh]:externalCall "External Call"':
          data:
            functionName: getMarkdownResult
            useErrorOutput: false
            useFunctionNameInput: false
          outgoingConnections:
            - result->"Graph Output" ZAnhfZqcJ5fwUEPZ_uqLl/value
          visualData: 2791.8598984550995/513.8687453142177/180/1309//
  metadata:
    description: ""
    id: GuX4N69d81LeoobNXiH0u
    title: talisman_test
  plugins:
    - id: anthropic
      name: Anthropic
      type: built-in
    - id: rivet-plugin-example-python-exec@latest
      package: rivet-plugin-example-python-exec
      tag: latest
      type: package
    - id: rivet-plugin-fs@latest
      package: rivet-plugin-fs
      tag: latest
      type: package
